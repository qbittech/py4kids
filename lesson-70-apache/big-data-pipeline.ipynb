{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/building-a-big-data-pipeline-with-airflow-spark-and-zeppelin-843f31ef220c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow\n",
    "\n",
    "\n",
    "### [QuickStart](https://airflow.apache.org/docs/stable/start.html)\n",
    "\n",
    "```\n",
    "$ sudo apt-get update\n",
    "$ sudo apt-get install build-essential\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# create a virtualenv\n",
    "$ cd ~/projects/bigdata\n",
    "$ python -m venv bigdata\n",
    "$ source bigdata/bin/activate\n",
    "\n",
    "mkdir ~/airflow\n",
    "mkdir ~/airflow/dags\n",
    "\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install apache-airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow initdb\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver -p 8080\n",
    "\n",
    "# start the scheduler\n",
    "airflow scheduler\n",
    "\n",
    "airflow list_dags\n",
    "airflow list_tasks tutorial\n",
    "airflow list_tasks tutorial --tree\n",
    "\n",
    "# validate\n",
    "python ~/airflow/dags/tutorial.py\n",
    "\n",
    "# test\n",
    "airflow test tutorial print_date 2015-06-01\n",
    "airflow test tutorial sleep 2015-06-01\n",
    "airflow test tutorial templated 2015-06-01\n",
    "\n",
    "# bash_operator\n",
    "python ~/airflow/dags/example_bash_operator.py list_tasks\n",
    "\n",
    "$ airflow list_tasks example_python_operator\n",
    "$ airflow test example_python_operator print_the_context 2020-01-01\n",
    "\n",
    "$ pip uninstall apache-airflow\n",
    " \n",
    "# postgresql\n",
    "$ sudo apt update\n",
    "$ sudo apt install postgresql postgresql-contrib\n",
    "$ service postgresql status\n",
    "\n",
    "$ sudo passwd postgres\n",
    "Enter new UNIX password: postgres\n",
    "Retype new UNIX password: postgres\n",
    " \n",
    "```\n",
    " \n",
    "### [Tutorial](https://airflow.apache.org/docs/stable/tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "\n",
    "- [Mastering Big Data with PySpark](https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark)\n",
    "\n",
    "- [Hands On Big Data Analytics with PySpark](https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark)\n",
    "\n",
    "- [apache-spark-for-big-data-analytics](https://medium.com/@christophberns/apache-spark-for-big-data-analytics-53b99185bf51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeppelin\n",
    "\n",
    "### Install\n",
    "https://zeppelin.apache.org/docs/0.7.3/install/install.html\n",
    "\n",
    "```\n",
    "$ tar -xvzf ~/Downloads/zeppelin-0.9.0-preview2-bin-all.tgz\n",
    "\n",
    "```\n",
    "\n",
    "### Run\n",
    "bin/zeppelin-daemon.sh start | stop | status\n",
    "\n",
    "goto Zeppelin Home Page at http://localhost:8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
